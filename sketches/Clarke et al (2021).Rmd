---
title: "Clarke et al (2021)"
subtitle: "Posttest-only 2 X 2 factorial design"
author: "A Solomon Kurz"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r, echo = F}
knitr::opts_chunk$set(fig.retina = 2.5)
options(width = 110)
```

Load our primary packages.

```{r, message = F, warning = F}
# library(tidyverse)

library(ggplot2)
library(tibble)
library(dplyr)
library(tidyr)
library(stringr)
library(readr)
library(forcats)
library(purrr)

library(brms)
library(tidybayes)
library(patchwork)
```

## Health warning labels for alcoholic beverages

Clarke and colleagues (2021; https://doi.org/10.1111/add.15072) reported the results from a posttest-only randomized experiment with a $2 \times 2$ factorial design to explore the impact of health warning labels (HWLs) on a alcohol-selection task. The authors made their data available on the OSF at https://osf.io/pr8zu/ (see the `Alcohol study 2 full dataset.xlsx` file in the `Study 2 folder`). However, given their primary outcome data are binomial, we can reproduce the full data set with a simple four-row tibble, based on the information they reported in their Table 2 (p. 46).

```{r}
clarke2021 <- tibble(
  group   = 1:4,
  image   = c(1, 0, 1, 0),
  text    = c(1, 1, 0, 0),
  hwl     = c("image and text", "text only", "image only", "control"),
  alcohol = c(837, 926, 728, 1157),
  total   = c(1501, 1511, 1502, 1510)) %>% 
  mutate(hwl = factor(hwl, levels = c("control", "image only", "text only", "image and text")))

# what is this?
print(clarke2021)
```

Clarke and colleagues recruited UK adults who consumed beer or whine at least one a week, with help from a market research agency (https://www.dynata.com/). The sample was designed to resemble the general UK adult population by age and gender. After consent, $n = 6{,}087$ participants were randomized into one of four conditions. After $n = 63$ dropped out (evenly dispersed across conditions), the four experimental groups and their sample sizes were:

* a no-HWL control ($n = 1510$),
* an image-only HWL group ($n = 1502$),
* a text-only HWL group ($n = 1511$), and
* an image-and-text HWL group ($n = 1501$).

These four groups are numbered as in the paper in the `group` variable and described with labels in the `hwl` factor variable. The groups are also described with the `image` and `text` dummy variables. 

After random assignment, participants viewed images of 6 alcoholic and 6 non-alcoholic drinks. The orders were randomized by participant and the labels on the drinks varied based on the experimental condition. After viewing all 12 images, participant were asked to choose which drink they would like to consume. Though Clarke and colleagues collected data on a variety of outcomes, their primary outcome was whether participants selected an alcoholic or non-alcoholic beverage. The `alcohol` column shows the numbers of participants who selected an alcoholic beverage in each group. The `total` columns shows the total sample size in each group.

## EDA

### Sample statistics.

Here are the sample statistics for the selection task, by group.

```{r}
clarke2021%>% 
  mutate(`chose non-alcoholic beverage` = total - alcohol) %>% 
  rename(`chose alcoholic beverage` = alcohol) %>% 
  select(group, hwl, `chose alcoholic beverage`, `chose non-alcoholic beverage`, total) %>% 
  mutate(`% chose alcohol` = round(100 * `chose alcoholic beverage` / total, digits = 1))
```

### Look at the data.

It might be nice to look at those statistics in a lollipop plot.

```{r, fig.width = 6, fig.height = 2}
clarke2021 %>% 
  mutate(label = str_c(alcohol, "/", total)) %>% 
  
  ggplot(aes(x = alcohol / total, y = hwl)) +
  geom_point(size = 3) +
  geom_linerange(aes(xmin = 0, xmax = alcohol / total)) +
  geom_text(aes(label = label, hjust = 0),
            nudge_x = .02, size = 3) +
  scale_x_continuous("proportion choosing alcohol (by HWL label)", 
                     labels = c("0", ".25", ".5", ".75", "1"),
                     expand = c(0, 0), limits = 0:1) +
  labs(y = NULL) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        panel.grid = element_blank())
```

## Models

As the data are simple bounded counts, a single-level binomial framework will handle them well. For the sake of pedagogy, we'll explore three variants of the same model. If we describe `alcohol` as varying across $i$ rows, we can describe the data with the two dummy variables and their interaction as

$$
\begin{align*}
\text{alcohol}_i          & \sim \operatorname{Binomial}(\text{total}_i, p_i) \\
\operatorname{logit}(p_i) & = \beta_0 + \beta_1 \text{image}_i + \beta_2 \text{text}_i + \beta_3 \text{image}_i \text{text}_i \\
\beta_0, \dots, \beta_3   & \sim \mathcal N(0, 1),
\end{align*}
$$

where the number of binomial trials for each condition is captured by the `total` variable. To constrain the conditional probabilities $p_i$ to within the $(0, 1)$ range, we use the conventional logit link. Given our two dummy variables `image` and `text`, $\beta_0$ captures the expected value for the reference category, the no-HWL control condition. $\beta_1$ captures the difference for the image-only HWL condition and $\beta_2$ captures the difference for the text-only HWL condition, both relative to the no-HWL control. Thus, $\beta_3$ is the interaction between the two dummies and the difference between the image-and-text HWL condition, relative to control, is captured by the combination of $\beta_1$, $\beta_2$, and $\beta_3$.

In their literature review (p. 42), Clarke and colleagues described the prior literature on HWL for alcoholic beverages as scant with small-$n$ studies which had somewhat contradictory findings. In this case, we might adopt a simple weakly-regularizing approach, rather than basing the priors on the findings of any specific study. By setting the prior for the control condition ($\beta_0$) to $\mathcal N(0, 1)$, we are centering the prior mass on the probability scale to .5, with a liberal 95% percentile range from .12 to .87.

```{r}
# prior scale
tibble(log_odds = rnorm(n = 1e5, mean = 0, sd = 1)) %>% 
  # probability scale
  mutate(p = inv_logit_scaled(log_odds)) %>% 
  pivot_longer(everything()) %>% 
  # summarize
  group_by(name) %>% 
  mean_qi(value)
```

The other $\beta$ parameters share the $\mathcal N(0, 1)$ prior, which gently centers the group differences from control to zero, but easily allows for large differences.

An alternative to the dummy-variable interaction model would be

$$
\begin{align*}
\text{alcohol}_i          & \sim \operatorname{Binomial}(\text{total}_i, p_i) \\
\operatorname{logit}(p_i) & = \beta_0 + \beta_1 \text{image-only}_i + \beta_2 \text{text-only}_i + \beta_3 \text{image-and-text}_i \\
\beta_0, \dots, \beta_3   & \sim \mathcal N(0, 1),
\end{align*}
$$

where we have replaced the `image` and `text` dummy variables and their interaction with three new dummy variables. Here `image only` is coded 1 for the image-only HWL condition and 0 otherwise, `text only` is coded 1 for the text-only HWL condition and 0 otherwise, and `image and text` is coded 1 for the image-and-text HWL condition and 0 otherwise. With such a model, $\beta_0$ still captures the expected value for the reference category, the no-HWL control condition. However, now the three remaining conditions are each expressed as deviations from the control condition without the need for an interaction term. As to priors, we continue to use the weakly-regularizing $\mathcal N(0, 1)$ for all $\beta$ parameters.

For the final model, we adopt a one-hot dummy coding system:

$$
\begin{align*}
\text{alcohol}_i          & \sim \operatorname{Binomial}(\text{total}_i, p_i) \\
\operatorname{logit}(p_i) & = \beta_0 \text{control}_i + \beta_1 \text{image-only}_i + \beta_2 \text{text-only}_i + \beta_3 \text{image-and-text}_i \\
\beta_0, \dots, \beta_3   & \sim \mathcal N(0, 1),
\end{align*}
$$

where now we add a `control` dummy to $\beta_0$. Provided `control` is coded 1 for the no-HWL condition and 0 otherwise, the $\beta_0$ parameter has the same meaning as in the other to models. But now we have included dummy variables for all four conditions, the remain $\beta_1$ through $\beta_3$ parameters are no longer expressed as deviations from the reference category. Rather, each $\beta$ coefficient stands for the expected value for its own condition. Once again, assigning the $\mathcal N(0, 1)$ prior to each parameter will weakly regularize the group probabilities toward .5.

From a likelihood perspective, all three versions of the model are equivalent. But because of how the $\beta$ parameters do or do not reference one another within each model, the common $\mathcal N(0, 1)$ will have slightly different effects on the posteriors. I would like to suggest that no one model is better than the others, but different researchers may find it easier to set the priors for one or another.

Anyway, here's how to fit the three models with `brm()`.

```{r fit1}
fit1 <- brm(
  data = clarke2021,
  family = binomial,
  alcohol | trials(total) ~ 0 + Intercept + image + text + image : text,
  prior = c(prior(normal(0, 1), class = b)),
  cores = 4, seed = 1,
  file = "fits/fit1.clarke2021"
)

fit2 <- brm(
  data = clarke2021,
  family = binomial,
  alcohol | trials(total) ~ 0 + Intercept + hwl,
  prior = c(prior(normal(0, 1), class = b)),
  cores = 4, seed = 1,
  file = "fits/fit2.clarke2021"
)

fit3 <- brm(
  data = clarke2021,
  family = binomial,
  alcohol | trials(total) ~ 0 + hwl,
  prior = c(prior(normal(0, 1), class = b)),
  cores = 4, seed = 1,
  file = "fits/fit3.clarke2021"
)
```

Check the summaries.

```{r}
summary(fit1)
summary(fit2)
summary(fit3)
```

As expected, the summaries for $\beta_0$ are about the same across models and the summaries for the other parameters differ quite a bit. Perhaps less interestingly, notice how the bulk and tail effective sample sizes are notable larger for the one-hot version of the model (`fit3`). You can get a sense of why by looking at the correlatinos among the parameters via `vcov()`.

```{r}
vcov(fit1, correlation = T) %>% round(digits = 3)
vcov(fit2, correlation = T) %>% round(digits = 3)
vcov(fit3, correlation = T) %>% round(digits = 3)
```

The first two versions of the model results in strong correlations among the $\beta$ parameters. Those strong correlations dropped toward zero with the one-hot coded model `fit3`. As a consequence, the effective sample size estimates shot up. The effective sample sizes were still large enough for the other two models that I wouldn't worry about using their results. But if you ever run into low effective sample size difficulties in your own research, perhaps a simple reparameterization could help.

As to the $\beta$ posteriors themselves, the advantage of the first two models is $\beta_1$ and $\beta_2$ both return odds ratios for the image-only and text-only conditions, relative to the no HWL control. Just exponentiate.

```{r}
fixef(fit1)[2:3, -2] %>% exp()
fixef(fit2)[2:3, -2] %>% exp()
```

As it is an interaction term, the $\beta_3$ posterior for `fit1` is difficult to interpret directly. However, researchers used to the NHST approach may appreciate how one can use the 95% intervals for the parameter to test the statistical significance of the interaction term. $\beta_3$ in `fit2`, however, can provide a straightforward odds ratio for the image-and-text HWL condition relative to control, after exponentiation.

```{r}
fixef(fit2)[4, -2] %>% exp()
```

The parameters in the one-hot based `fit3` do not lend themselves as easily to odds ratios. However, one can work directly with the posterior draws from any of the three models to hand-compute odds ratios or any other comparisons among the conditions, as desired. To my mind, the `brms::fitted()` function provides the easiest method for wrangling the posteriors. To give a sense, here we use `fitted()` to compute the log-odds for each condition, from each of the three versions of the model.

```{r}
# fit1
f1 <- fitted(
  fit1,
  scale = "linear",
  summary = F
) %>% 
  data.frame() %>% 
  set_names(pull(clarke2021, hwl)) %>% 
  mutate(draw = 1:n(),
         fit = "fit1")

f2 <- fitted(
  fit2,
  scale = "linear",
  summary = F
) %>% 
  data.frame() %>% 
  set_names(pull(clarke2021, hwl)) %>% 
  mutate(draw = 1:n(),
         fit = "fit2")

f3 <- fitted(
  fit3,
  scale = "linear",
  summary = F
) %>% 
  data.frame() %>% 
  set_names(pull(clarke2021, hwl)) %>% 
  mutate(draw = 1:n(),
         fit = "fit3")
  
# combine
f <- bind_rows(f1, f2, f3) %>% 
  # make it long
  pivot_longer(`image and text`:control, names_to = "hwl", values_to = "log_odds") %>% 
  # convert log odds to probabilities
  mutate(p = inv_logit_scaled(log_odds))

# what?
glimpse(f)
```

With all the posterior draws across the three models in one `f` object, we can display the conditional probabilities in a coefficient plot.

```{r}
p1 <- f %>% 
  mutate(hwl = factor(hwl, levels = clarke2021 %>% arrange(hwl) %>% pull(hwl))) %>% 
  
  ggplot(aes(x = p, y = hwl)) +
  stat_pointinterval(aes(color = fit, group = fit),
                     .width = .95, size = 1.5, position = position_dodge(width = -0.6)) +
  scale_color_viridis_d(NULL, option = "B", begin = .1, end = .6, direction = -1) +
  scale_x_continuous("probability of choosing alcohol", 
                     labels = c("0", ".25", ".5", ".75", "1"),
                     expand = c(0, 0), limits = 0:1) +
  labs(title = "Population estimates (by HWL)",
       y = NULL) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        panel.grid = element_blank())
```


With aid from the `tidybayes::compare_levels()` function, it's easy to compute all pairwise effect sizes as probability contrasts and then display the results in a coefficient plot.

```{r}
# for the y-axis labels
group_labels <- c("image and text", "text only", "image only", "control")

contrast_levels <- c("image only \u2212 control", "text only \u2212 control", "image and text \u2212 control", "text only \u2212 image only", "image and text \u2212 image only", "image and text \u2212 text only")

# wrangle
p2 <- f %>% 
  mutate(hwl = factor(hwl, levels = group_labels)) %>%
  mutate(hwl = fct_rev(hwl)) %>%
  compare_levels(p, by = hwl, draw_indices = c("draw", "fit")) %>% 
  mutate(hwl = str_replace(hwl, "-", "\u2212")) %>% 
  mutate(hwl = factor(hwl, levels = contrast_levels))  %>% 
  
  # plot!
  ggplot(aes(x = p, y = hwl)) +
  stat_pointinterval(aes(color = fit, group = fit),
                     .width = .95, size = 1.5, position = position_dodge(width = -0.6)) +
  scale_color_viridis_d(NULL, option = "B", begin = .1, end = .6, direction = -1) +
  scale_x_continuous("difference in probability of choosing alcohol", 
                     labels = c("-.5", "-.25", "0", ".25", ".5"),
                     limits = c(-0.5, 0.5)) +
  labs(title = "Pairwise contrasts among HWLs",
       y = NULL) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        panel.grid = element_blank())
```

Here we combine the two sub plots, add an overall title, and display the results.

```{r, fig.width = 9, fig.height = 4.75}
p1 / p2 + 
  plot_annotation(title = "Health warning labels (HWLs) reduce alcoholic beverage selection, compared to a no-HWL control.") +
  plot_layout(heights = c(2, 3), guides = "collect")
```

You'll note that although the probabilities and their contrasts are very similar across the three models, there are subtle differences, particularly for `fit3`. As we discussed before, the $\mathcal N(0, 1)$ prior imposed slightly different consequences for the marginal posteriors in each model.

## Session info

```{r}
sessionInfo()
```

